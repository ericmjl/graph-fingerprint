{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wb import WeightsAndBiases\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from random import sample, choice\n",
    "from fingerprint_vect import GraphFingerprint\n",
    "from collections import defaultdict\n",
    "from autograd import grad\n",
    "from autograd.scipy.misc import logsumexp\n",
    "\n",
    "import autograd.numpy as np\n",
    "import networkx as nx\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_random_graph(nodes, n_edges, features_dict):\n",
    "    \"\"\"\n",
    "    Makes a randomly connected graph. \n",
    "    \"\"\"\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    for n in nodes:\n",
    "        G.add_node(n, features=features_dict[n])\n",
    "    \n",
    "    for i in range(n_edges):\n",
    "        u, v = sample(G.nodes(), 2)\n",
    "        G.add_edge(u, v)\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 7, {}), (1, 9, {}), (2, 9, {}), (2, 7, {})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features_dict will look like this:\n",
    "# {0: array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "#  1: array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "#  2: array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
    "#  3: array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0]),\n",
    "#  4: array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
    "#  5: array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0]),\n",
    "#  6: array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
    "#  7: array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
    "#  8: array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0]),\n",
    "#  9: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])}\n",
    "\n",
    "all_nodes = [i for i in range(10)]    \n",
    "lb = LabelBinarizer()\n",
    "features_dict = {i:lb.fit_transform(all_nodes)[i] for i in all_nodes}\n",
    "\n",
    "G = make_random_graph(sample(all_nodes, 6), 5, features_dict)\n",
    "G.edges(data=True)\n",
    "# G.nodes(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'biases': array([[-0.01282643,  0.05126014,  0.12478795, -0.04467987, -0.23225055,\n",
       "           0.05331752, -0.00878325,  0.02578893, -0.12787258,  0.02081464,\n",
       "          -0.09019882,  0.13098511,  0.05393008, -0.06729331,  0.09554677,\n",
       "           0.02914731,  0.01288287, -0.04030879,  0.01075938,  0.04975741]]),\n",
       "  'nbr_weights': array([[-0.03632008,  0.0040548 ,  0.00716803,  0.01196963,  0.20626901,\n",
       "           0.0655909 , -0.05097722, -0.10702418,  0.05852709, -0.0010574 ,\n",
       "           0.09959072,  0.03393232, -0.00762132,  0.15056993, -0.05630408,\n",
       "           0.03298202,  0.01378147,  0.08816432, -0.12404468,  0.06024068],\n",
       "         [-0.08993807, -0.1001855 , -0.0840236 , -0.18067154, -0.04968924,\n",
       "           0.04929193,  0.08026344,  0.16582503, -0.24756252,  0.0354438 ,\n",
       "           0.00573073, -0.04362545,  0.01296043,  0.03062234, -0.16566107,\n",
       "           0.07478288, -0.00085774, -0.09147302, -0.14658804, -0.06035202],\n",
       "         [ 0.05898323,  0.13520439, -0.09526349,  0.02845017,  0.03359888,\n",
       "           0.02376599, -0.17603841,  0.05816554, -0.05802099, -0.09431107,\n",
       "           0.04251198,  0.07525588, -0.0837717 ,  0.09038241, -0.08261424,\n",
       "           0.00901072, -0.14941068, -0.10608827, -0.11697244, -0.12196637],\n",
       "         [-0.03510305,  0.19647544, -0.0632154 , -0.07866361, -0.08622205,\n",
       "          -0.01228424,  0.02554525, -0.04783861, -0.11861233,  0.10254431,\n",
       "           0.03144783,  0.07771415,  0.15076451,  0.10237987,  0.01004333,\n",
       "          -0.05713008, -0.05423807, -0.04087578,  0.04499361,  0.10038523],\n",
       "         [ 0.06753583, -0.11439008,  0.03511252,  0.20170161, -0.08458585,\n",
       "           0.11953081,  0.06769274,  0.04928663,  0.01373566,  0.06202304,\n",
       "           0.12539413, -0.06000059, -0.09770157, -0.02554416, -0.03113212,\n",
       "           0.06258891,  0.12608289,  0.02888359, -0.0200097 , -0.0693534 ],\n",
       "         [-0.11186575,  0.05443002,  0.01317365, -0.05175881,  0.05099281,\n",
       "          -0.12861847, -0.00809726, -0.07551549,  0.17867292,  0.12125348,\n",
       "          -0.01450656,  0.03806883, -0.15707789,  0.14821799,  0.25930455,\n",
       "          -0.07963401,  0.13883856,  0.06301745, -0.0961949 ,  0.03034843],\n",
       "         [ 0.06195235, -0.08175998,  0.01425198, -0.03580336, -0.19691898,\n",
       "          -0.0022962 ,  0.06128355, -0.02906682,  0.08856138, -0.02183622,\n",
       "          -0.03717781, -0.00102078,  0.25097835, -0.02075407, -0.03350454,\n",
       "          -0.10291089,  0.09167559, -0.00944332, -0.28666359,  0.00360039],\n",
       "         [ 0.19325667, -0.02282261, -0.02086349, -0.13519001,  0.02222449,\n",
       "           0.07561906,  0.23070887, -0.08795912, -0.01872063, -0.05047235,\n",
       "           0.00776788,  0.04417061, -0.08985975, -0.03399167, -0.00997196,\n",
       "           0.21060341, -0.04834938, -0.16264752, -0.02388926,  0.06096059],\n",
       "         [ 0.03511144,  0.10533312, -0.03436835, -0.10944401, -0.04588894,\n",
       "           0.09670492, -0.09418594,  0.24886949, -0.06553092,  0.10627361,\n",
       "          -0.05694096,  0.10348631,  0.02384301, -0.14986613, -0.01019134,\n",
       "          -0.08025894,  0.2657085 ,  0.08799265,  0.05328658,  0.2507771 ],\n",
       "         [ 0.03970082,  0.04974226,  0.01105009,  0.02736716,  0.11759658,\n",
       "           0.08572447, -0.03697642, -0.00352698,  0.06150993,  0.13641659,\n",
       "           0.02708937, -0.0579704 , -0.07149478,  0.10087582,  0.0380517 ,\n",
       "          -0.06621459, -0.04808597,  0.0568058 ,  0.08079211,  0.10036365]]),\n",
       "  'self_weights': array([[ -9.01960225e-02,  -3.75994449e-02,  -9.25561888e-02,\n",
       "            4.96232005e-05,   7.58239712e-02,  -1.52052158e-01,\n",
       "           -5.99242167e-02,  -9.87776096e-03,   8.13569926e-03,\n",
       "           -7.98852776e-02,   2.91912104e-02,   4.58331846e-02,\n",
       "            6.02852393e-02,  -1.81054492e-01,   3.77108613e-03,\n",
       "            4.70385149e-02,   8.07015869e-02,  -1.56624891e-01,\n",
       "           -1.00632728e-01,   4.59575235e-02],\n",
       "         [  1.48935602e-01,  -6.74436939e-02,   3.93411295e-02,\n",
       "            8.81194867e-02,  -1.95764605e-01,   2.42870525e-02,\n",
       "            3.44802778e-02,  -2.30204513e-01,  -7.81812681e-02,\n",
       "           -2.76300797e-02,  -1.18156179e-01,  -5.77731476e-02,\n",
       "           -2.58964736e-01,  -3.87597649e-02,   4.44202588e-02,\n",
       "            6.68463801e-02,   7.81962935e-02,  -2.71469430e-01,\n",
       "           -2.01936350e-01,   3.15441705e-02],\n",
       "         [ -6.63718751e-02,  -1.91121517e-01,   3.52738105e-02,\n",
       "            5.07516970e-02,  -1.42420696e-01,  -8.27607295e-02,\n",
       "           -6.46594650e-02,   5.46034124e-03,  -1.84412302e-01,\n",
       "            9.09647172e-03,  -9.44570329e-02,  -2.26437803e-02,\n",
       "            2.28116010e-01,  -6.30209787e-02,   3.06106332e-02,\n",
       "           -1.68046409e-01,   8.78784396e-02,   9.35082948e-02,\n",
       "            1.81738544e-02,  -8.79886816e-02],\n",
       "         [  7.09112445e-02,   1.49346587e-01,  -8.09201207e-02,\n",
       "            8.46862689e-02,   2.52719270e-01,  -2.42675677e-02,\n",
       "           -8.93646507e-02,  -1.52085503e-01,   1.54347207e-02,\n",
       "           -1.09406924e-01,  -1.60706947e-01,   6.27673388e-02,\n",
       "            5.16945330e-02,  -1.30695565e-02,  -1.86613993e-01,\n",
       "           -1.55759960e-02,  -3.39501512e-02,  -4.33333295e-03,\n",
       "            9.58284711e-02,  -1.66263104e-01],\n",
       "         [  4.38260897e-03,   1.16556499e-01,   5.92230760e-02,\n",
       "           -3.30869014e-02,  -1.20892612e-01,   3.57163591e-02,\n",
       "            8.77866328e-02,   6.42667842e-02,  -6.44943659e-02,\n",
       "           -1.22594812e-01,   1.37683610e-01,  -1.20803422e-03,\n",
       "           -5.55529099e-02,   5.86163887e-02,   1.08802990e-02,\n",
       "            1.97672644e-01,   3.31315252e-02,  -1.42588635e-01,\n",
       "           -1.94382235e-01,   7.26771073e-03],\n",
       "         [ -1.00476880e-01,   7.74987011e-02,   3.62469440e-02,\n",
       "           -1.59894187e-01,   1.74594021e-01,   2.80124155e-01,\n",
       "           -2.62946592e-03,  -4.63313454e-02,   9.89706123e-02,\n",
       "           -5.86830497e-02,  -5.53511688e-02,   3.64682924e-02,\n",
       "            3.27586994e-02,  -2.33293208e-02,  -1.02588762e-01,\n",
       "            5.46723313e-03,   1.42720394e-01,  -7.21465841e-02,\n",
       "           -9.38916087e-02,   1.20991443e-01],\n",
       "         [  1.07994917e-01,  -1.94505261e-02,  -6.06390825e-03,\n",
       "           -1.76019622e-02,   3.85490710e-02,   2.70738798e-01,\n",
       "            2.16328418e-01,  -1.74898072e-02,  -1.09856282e-01,\n",
       "           -1.86049961e-01,   7.99553908e-02,   1.40835371e-01,\n",
       "            1.17689224e-01,   5.24401181e-02,  -1.86941166e-01,\n",
       "           -1.41648390e-01,  -1.40568192e-01,   6.70152026e-03,\n",
       "           -1.88405895e-01,  -3.15389524e-02],\n",
       "         [  1.68613759e-02,  -8.25028455e-02,  -1.03218652e-01,\n",
       "           -1.45693363e-02,   1.78841944e-01,   1.71746996e-01,\n",
       "            9.10211348e-03,  -7.71002277e-02,   4.21006422e-02,\n",
       "           -4.62752915e-02,  -6.03347449e-02,   2.29162239e-02,\n",
       "            1.15668484e-01,   1.12416619e-01,   7.88362385e-02,\n",
       "           -5.77680963e-02,  -9.25698459e-02,  -2.44570103e-01,\n",
       "           -5.18509237e-02,   4.99813302e-02],\n",
       "         [ -9.70897137e-02,  -4.30025129e-02,  -1.40682934e-01,\n",
       "           -4.65055144e-02,   2.02799497e-01,  -1.51852149e-01,\n",
       "            6.55040867e-02,   7.07638578e-03,  -4.03393745e-02,\n",
       "            4.08645263e-02,  -5.20182179e-02,   7.40499454e-03,\n",
       "            1.33705945e-01,  -3.64915558e-02,   1.60336635e-01,\n",
       "           -8.04703702e-03,  -5.80303912e-02,  -5.96605686e-02,\n",
       "            1.76150183e-01,  -5.03825923e-03],\n",
       "         [  3.46146848e-02,   7.97117464e-02,  -1.16724509e-01,\n",
       "           -9.74222363e-03,   5.60144908e-02,   3.61709401e-02,\n",
       "            9.03824809e-02,   6.76805239e-02,  -1.88035054e-01,\n",
       "            7.91198944e-02,   3.52516191e-02,   2.74323698e-02,\n",
       "            1.32291947e-01,   4.49397552e-02,   4.35008004e-02,\n",
       "            1.83095799e-01,  -1.92842200e-01,  -2.12846537e-03,\n",
       "            8.26388009e-03,   1.34680737e-01]])},\n",
       " 1: {'biases': array([[ 0.02186266, -0.0460932 ,  0.05321547,  0.07076782, -0.00560358,\n",
       "           0.15835266,  0.1841183 ,  0.05971739, -0.11932465,  0.00892916]]),\n",
       "  'nbr_weights': array([[ -6.26513626e-02,   4.86892940e-02,  -1.54294741e-01,\n",
       "           -7.62216283e-02,  -4.98192607e-02,   3.46556613e-02,\n",
       "            1.41995980e-01,  -2.18642031e-03,   1.53375559e-01,\n",
       "            7.86067881e-02],\n",
       "         [ -1.04959557e-01,   1.13143258e-02,   1.03484965e-02,\n",
       "            3.25637327e-02,   2.51360289e-01,  -2.58530537e-02,\n",
       "            1.36042021e-01,   1.37010787e-02,   5.77093538e-03,\n",
       "            1.42544484e-02],\n",
       "         [ -1.17480002e-01,   3.00368042e-02,   1.25288013e-01,\n",
       "           -8.32089294e-02,   1.16452687e-01,  -1.25701839e-01,\n",
       "           -4.30927866e-02,  -3.07586208e-02,   4.88438433e-02,\n",
       "            1.09995365e-01],\n",
       "         [  3.99640030e-02,  -4.54249267e-02,  -5.19741518e-02,\n",
       "           -4.17490912e-02,   8.08597826e-02,   2.21939999e-01,\n",
       "           -4.43211183e-03,   2.55808848e-01,  -1.02641518e-01,\n",
       "           -1.14052973e-01],\n",
       "         [  8.83820783e-02,   4.96259151e-02,   1.49468308e-01,\n",
       "            2.68574879e-02,   7.21216133e-02,   2.15234648e-01,\n",
       "            1.17021501e-02,   5.20300826e-02,  -1.36370584e-01,\n",
       "            6.56418440e-02],\n",
       "         [ -4.90303459e-02,  -6.14559841e-03,   8.18525098e-02,\n",
       "           -8.37476981e-03,  -4.15654461e-02,  -3.87544897e-02,\n",
       "            3.63857152e-02,  -5.19691003e-02,   1.11796754e-02,\n",
       "           -1.06684795e-01],\n",
       "         [  5.31897486e-03,  -7.30543084e-03,  -1.41338949e-01,\n",
       "            5.67568134e-02,  -4.56437788e-02,  -2.54218338e-02,\n",
       "            1.04854689e-02,  -1.52469967e-02,  -3.54063670e-02,\n",
       "            9.86648884e-03],\n",
       "         [  1.31523980e-03,  -5.38530299e-02,   1.96622929e-01,\n",
       "            1.81537206e-02,  -2.19343607e-02,   1.07730052e-01,\n",
       "           -1.11150690e-01,  -5.57068646e-02,   2.10306064e-01,\n",
       "           -1.71689081e-01],\n",
       "         [  1.00714789e-02,   6.01202534e-02,  -1.32195069e-01,\n",
       "            6.92685312e-02,  -1.85646515e-01,   1.01967885e-01,\n",
       "            1.24513847e-01,   8.54202004e-02,  -4.56616135e-02,\n",
       "            1.51327245e-02],\n",
       "         [  1.04751970e-02,  -2.00033417e-02,   1.59329152e-01,\n",
       "            3.50290082e-02,  -1.73938234e-01,  -1.19871096e-01,\n",
       "           -4.06132125e-02,  -4.77795542e-02,   9.96138268e-02,\n",
       "            5.18885545e-02],\n",
       "         [ -9.75389752e-02,  -1.81838683e-01,   8.76288367e-02,\n",
       "           -7.34598530e-03,   1.70625252e-01,  -5.66072689e-02,\n",
       "           -5.99467040e-02,  -1.38883696e-01,  -1.45472537e-01,\n",
       "           -4.96857129e-02],\n",
       "         [ -1.17692567e-01,  -1.29114986e-01,  -1.39740962e-03,\n",
       "            1.86490807e-01,   4.62104906e-02,   1.30361924e-01,\n",
       "            9.94127209e-02,   1.46029446e-02,  -1.76803203e-02,\n",
       "            5.64018442e-02],\n",
       "         [  6.47190288e-02,   4.57868970e-03,   9.55617698e-02,\n",
       "            5.27633211e-02,  -2.69385636e-02,  -7.70282062e-02,\n",
       "           -1.96107266e-01,  -1.90105277e-01,  -1.82490730e-02,\n",
       "           -4.48745550e-02],\n",
       "         [  2.53642447e-01,  -4.67489210e-02,   4.12068935e-02,\n",
       "            4.52052693e-02,  -9.56346412e-02,  -9.36082116e-02,\n",
       "            1.04160298e-01,  -1.44632128e-01,   7.79770626e-02,\n",
       "           -2.83624795e-01],\n",
       "         [  1.25076566e-01,   4.59700304e-03,   1.99417864e-02,\n",
       "            2.36113082e-01,  -5.30360330e-02,  -1.73237547e-01,\n",
       "            1.63850466e-01,  -3.99881288e-02,   1.32523194e-01,\n",
       "            8.35557715e-02],\n",
       "         [  8.05204535e-02,   1.04059187e-01,  -6.88578674e-02,\n",
       "           -1.12697948e-02,  -2.78140078e-02,  -2.89648088e-02,\n",
       "            7.11915584e-02,   1.58999668e-01,   9.24515557e-03,\n",
       "           -7.64857173e-02],\n",
       "         [  7.79587466e-02,   1.08639986e-03,   5.63790741e-02,\n",
       "           -3.35053884e-02,   5.40970126e-02,  -4.99522718e-02,\n",
       "            1.29217873e-02,   1.00919796e-01,   1.65959358e-01,\n",
       "            7.18877605e-02],\n",
       "         [  4.61388834e-02,   1.65432582e-01,  -1.96126170e-01,\n",
       "           -3.24226207e-02,  -1.23131084e-01,  -8.96650650e-02,\n",
       "            2.64854773e-04,  -1.83450737e-01,  -5.59318764e-02,\n",
       "           -1.58667892e-01],\n",
       "         [  1.21361629e-01,   1.28384358e-01,  -1.04125623e-01,\n",
       "            4.08833430e-02,   6.93754274e-03,   4.90713304e-02,\n",
       "            1.70875217e-01,   1.47083840e-01,  -6.20397392e-02,\n",
       "           -5.90982157e-02],\n",
       "         [ -6.65573957e-03,  -1.85303517e-02,   6.46316236e-02,\n",
       "           -1.31409253e-01,  -1.27759681e-01,  -1.13702646e-01,\n",
       "            1.17394241e-02,  -1.00889121e-01,  -1.13325623e-01,\n",
       "           -1.01962681e-01]]),\n",
       "  'self_weights': array([[-0.0962002 ,  0.11043911, -0.06436455,  0.05148998, -0.08215578,\n",
       "          -0.00662056, -0.08867035,  0.02885557,  0.15693185, -0.00725244],\n",
       "         [ 0.13507082, -0.01453751, -0.03787736,  0.10964525, -0.21738166,\n",
       "          -0.13265238,  0.06307887, -0.14419024, -0.04995215,  0.04534652],\n",
       "         [-0.14418121,  0.10980884,  0.23353432, -0.11753855, -0.01357034,\n",
       "           0.01434604,  0.13088898, -0.000825  ,  0.18801988,  0.04968455],\n",
       "         [-0.13454377, -0.05887133,  0.04797912,  0.12270093,  0.11344506,\n",
       "          -0.05611199,  0.05534186, -0.1073956 , -0.05766685,  0.09355195],\n",
       "         [-0.03347277,  0.031095  ,  0.02562486,  0.11415197, -0.11810866,\n",
       "          -0.150216  ,  0.01935035,  0.05671151, -0.15147968, -0.19435052],\n",
       "         [-0.09729211, -0.2239216 ,  0.02979215,  0.02640663, -0.05378034,\n",
       "           0.07348137, -0.12268291, -0.07722973, -0.04005624, -0.09239614],\n",
       "         [-0.07503261, -0.08800143,  0.0219313 ,  0.13160545, -0.11880953,\n",
       "           0.07527024,  0.0672646 ,  0.22960996, -0.07769181, -0.01323895],\n",
       "         [-0.01923081, -0.10260097, -0.09759713,  0.04904909,  0.1156645 ,\n",
       "           0.01306693, -0.03004013,  0.00832677,  0.07138049, -0.04937047],\n",
       "         [-0.17646254,  0.06605384, -0.22774778,  0.02233748, -0.03451803,\n",
       "           0.01030975,  0.15781548, -0.1148551 ,  0.15088487,  0.06008444],\n",
       "         [ 0.09932704, -0.00476291, -0.26859932,  0.15476665, -0.10554374,\n",
       "           0.10240276, -0.02214869,  0.02876079, -0.07777364,  0.03244053],\n",
       "         [-0.01719342, -0.11187006,  0.00837868,  0.03265317, -0.14044828,\n",
       "           0.10376031,  0.07573629,  0.08660057,  0.11391621,  0.25718346],\n",
       "         [ 0.12183077, -0.08687949, -0.02876068, -0.01182417, -0.15890864,\n",
       "           0.00906536, -0.10439043, -0.14548165, -0.09946971, -0.03344754],\n",
       "         [-0.05413761, -0.06437497,  0.07819716,  0.02502082,  0.11392779,\n",
       "          -0.07222899, -0.09424622,  0.02339854, -0.14097638, -0.08849592],\n",
       "         [-0.06584742,  0.02767327, -0.08201895,  0.10794261, -0.01762517,\n",
       "          -0.09409028,  0.02951061,  0.01710839,  0.09646856, -0.06418203],\n",
       "         [ 0.03314157,  0.02098987,  0.11595717,  0.04988003, -0.04398134,\n",
       "           0.16069591, -0.00930871, -0.00263934,  0.05561243, -0.01558943],\n",
       "         [-0.05729489,  0.06041855,  0.10009091, -0.05995652,  0.03761297,\n",
       "          -0.05135869,  0.00182022,  0.06146269,  0.09595158, -0.07549837],\n",
       "         [-0.04772161, -0.05794187,  0.04156748, -0.12705588,  0.1627808 ,\n",
       "          -0.00644607, -0.04703659, -0.03074461, -0.1244818 , -0.00052028],\n",
       "         [-0.13242625,  0.06130764,  0.10620099,  0.00922704,  0.05939076,\n",
       "          -0.06319252, -0.05142597, -0.05479808,  0.02564276,  0.119129  ],\n",
       "         [-0.15580357, -0.11474194, -0.02451398, -0.00665232, -0.08253168,\n",
       "          -0.03381447, -0.00529444, -0.27428986, -0.191055  , -0.17938615],\n",
       "         [-0.06841974,  0.06364916, -0.03472491,  0.02016813, -0.11235553,\n",
       "          -0.14579847, -0.16507754, -0.14888093, -0.04276396,  0.02372869]])},\n",
       " 2: {'biases': array([[ 0.11901306, -0.04708679,  0.12233301, -0.03564955, -0.17132527,\n",
       "          -0.07616449, -0.05509415,  0.08467867,  0.26847314,  0.03476677]]),\n",
       "  'linweights': array([[-0.09248054],\n",
       "         [-0.08554026],\n",
       "         [-0.00312344],\n",
       "         [-0.02349777],\n",
       "         [ 0.14925683],\n",
       "         [-0.03304639],\n",
       "         [-0.00561274],\n",
       "         [-0.12485762],\n",
       "         [ 0.02932818],\n",
       "         [ 0.09076889]]),\n",
       "  'nbr_weights': array([[-0.03443231, -0.10297938,  0.00674863, -0.09623424,  0.09333298,\n",
       "          -0.06397167, -0.03375024,  0.02593779, -0.1483537 , -0.09655459],\n",
       "         [ 0.17655639, -0.07257145,  0.12863431, -0.23796243, -0.09509387,\n",
       "           0.02886162,  0.07959682, -0.06303253, -0.0718766 , -0.0350379 ],\n",
       "         [-0.01448031, -0.01012487, -0.09798019, -0.05445151,  0.01349449,\n",
       "           0.14438452, -0.09814525,  0.16026005, -0.02237929, -0.10454445],\n",
       "         [-0.03996543,  0.09489398, -0.15142372,  0.16240258, -0.04457266,\n",
       "          -0.02323181,  0.0385138 ,  0.05465601, -0.09567448, -0.02273267],\n",
       "         [-0.17227907,  0.07816039,  0.07787005,  0.0484548 , -0.09251387,\n",
       "           0.13416743,  0.03889152, -0.01198443, -0.10048614, -0.03798744],\n",
       "         [-0.02727398, -0.02992318,  0.09540499, -0.01855012,  0.0022896 ,\n",
       "           0.05422299, -0.10338521,  0.1510383 ,  0.04085092,  0.04158431],\n",
       "         [ 0.24898686, -0.12110065,  0.14646451,  0.04292987, -0.04419865,\n",
       "          -0.02544003, -0.00589144, -0.11809923,  0.01504251,  0.04477492],\n",
       "         [-0.09158916, -0.03245829,  0.03404797,  0.01605868, -0.04359475,\n",
       "          -0.01771952,  0.13428792,  0.13832816, -0.20461097,  0.20416112],\n",
       "         [-0.07149898,  0.07991436,  0.10389143,  0.17104467,  0.16686339,\n",
       "          -0.17894135, -0.15313617, -0.15150783,  0.16466393,  0.01956815],\n",
       "         [ 0.14151711,  0.05859194,  0.08297057, -0.03896693, -0.02677758,\n",
       "          -0.17634067, -0.15847718, -0.04159392, -0.21259733, -0.10290795]]),\n",
       "  'self_weights': array([[-0.02101582,  0.0019339 , -0.03483594,  0.07825127,  0.04142297,\n",
       "           0.15673261, -0.21020469, -0.14451419, -0.11314683,  0.06138482],\n",
       "         [-0.01529662,  0.02363775, -0.0253688 , -0.06450503, -0.12379467,\n",
       "           0.17040402,  0.11925215, -0.1000468 , -0.03226223,  0.19928495],\n",
       "         [ 0.14929754,  0.02803383, -0.02775755,  0.10429484,  0.03896381,\n",
       "           0.02642051, -0.01931416, -0.05378883,  0.08188825, -0.11283168],\n",
       "         [ 0.04907164, -0.04418396,  0.05434429, -0.11748211, -0.03229132,\n",
       "          -0.01334779, -0.06967456,  0.01986687, -0.04959126, -0.03194063],\n",
       "         [ 0.09504948, -0.09269342,  0.0051121 , -0.14430831,  0.00482644,\n",
       "          -0.11662835, -0.07057459, -0.09429744,  0.14579817,  0.02762993],\n",
       "         [-0.01577976, -0.00221053, -0.0748042 ,  0.18042225,  0.12836725,\n",
       "          -0.01049622, -0.03478032,  0.0446146 ,  0.08234012,  0.09436787],\n",
       "         [-0.08071518,  0.03121242,  0.10928861,  0.18363335,  0.10581681,\n",
       "          -0.12553332, -0.00025646, -0.03366692, -0.11851143,  0.06710427],\n",
       "         [-0.06986731, -0.18280698, -0.17732692, -0.03674395, -0.05288279,\n",
       "           0.00178499,  0.04895287,  0.08963836, -0.00810671,  0.02071877],\n",
       "         [-0.07594659,  0.03914949, -0.0052627 ,  0.03482906, -0.17527643,\n",
       "           0.06454122,  0.06138547,  0.0404627 , -0.11489428, -0.03278138],\n",
       "         [ 0.06543875, -0.00412772,  0.11086976, -0.01132996,  0.08693528,\n",
       "           0.09806873, -0.12326476, -0.06869782, -0.01672318, -0.05067624]])}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb = WeightsAndBiases(n_layers=2, shapes=(10, 20, 10))\n",
    "wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def score(G):\n",
    "#     \"\"\"\n",
    "#     The regressable score for each graph will be the sum of the \n",
    "#     (square root of each node + the sum of its neighbors.)\n",
    "#     \"\"\"\n",
    "#     sum_score = 0\n",
    "#     for n, d in G.nodes(data=True):\n",
    "#         sum_score += math.sqrt(n)\n",
    "        \n",
    "#         for nbr in G.neighbors(n):\n",
    "#             sum_score += nbr ** (1/3)\n",
    "#     return sum_score\n",
    "\n",
    "def score(G):\n",
    "    \"\"\"\n",
    "    The regressable score for each graph is the number of nodes\n",
    "    in the graph.\n",
    "    \"\"\"\n",
    "    return len(G.nodes())\n",
    "\n",
    "score(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.nodes(data=True)[0][1]['features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07224867,  0.1477318 ,  0.12377   ,  0.0666501 ,  0.12325732,\n",
       "         0.09957421,  0.13729153,  0.080011  ,  0.05878282,  0.09068256]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(X, axis=0):\n",
    "    \"\"\"\n",
    "    The softmax function normalizes everything to between 0 and 1.\n",
    "    \"\"\"\n",
    "    return np.exp(X - logsumexp(X, axis=axis, keepdims=True))\n",
    "\n",
    "# test softmax:\n",
    "X = np.random.random((1,10))\n",
    "softmax(X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.34396448]\n",
      " [-0.51544308]\n",
      " [ 0.12502438]\n",
      " [-0.35634757]\n",
      " [-0.77549162]]\n",
      "\n",
      "[[ 0.34396448]\n",
      " [-0.        ]\n",
      " [ 0.12502438]\n",
      " [-0.        ]\n",
      " [-0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def relu(X):\n",
    "    \"\"\"\n",
    "    The ReLU - Rectified Linear Unit.\n",
    "    \"\"\"\n",
    "    return X * (X > 0)\n",
    "\n",
    "\n",
    "# test relu:\n",
    "X = np.random.normal(0, 1, size=(5, 1))\n",
    "print(X)\n",
    "print('')\n",
    "print(relu(X)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make 1000 random graphs.\n",
    "syngraphs = []\n",
    "for i in range(100):\n",
    "    n_nodes = choice([i for i in range(2, 10)])\n",
    "    n_edges = choice([i for i in range(1, n_nodes**2)])\n",
    "    \n",
    "    G = make_random_graph(sample(all_nodes, n_nodes), n_edges, features_dict)\n",
    "    syngraphs.append(G)\n",
    "    \n",
    "len(syngraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write a function that computes the feature matrix, and writes the\n",
    "# indices to the nodes of each graph.\n",
    "def stacked_node_activations(graphs):\n",
    "    \"\"\"\n",
    "    Note: this function should only be called for computing the\n",
    "    stacked node activations after initializing the graphs.\n",
    "    \n",
    "    Inputs:\n",
    "    =======\n",
    "    - graphs: (list) a list of graphs on which to stack their\n",
    "              feature vectors.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    curr_idx = 0\n",
    "    for g in graphs:\n",
    "        for n, d in g.nodes(data=True):\n",
    "            features.append(d['features'])\n",
    "            g.node[n]['idx'] = curr_idx\n",
    "            curr_idx += 1\n",
    "    return np.vstack(features)\n",
    "\n",
    "# test stacked_node_activations\n",
    "layers = dict()\n",
    "layers[0] = stacked_node_activations(syngraphs)\n",
    "layers[1] = stacked_node_activations(syngraphs)\n",
    "# layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a function that gets the indices of each node's neighbors.\n",
    "def neighbor_indices(G, n):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    =======\n",
    "    - G: the graph to which the node belongs to.\n",
    "    - n: the node inside the graph G.\n",
    "    \n",
    "    Returns:\n",
    "    ========\n",
    "    - indices: (list) a list of indices, which should (but is not\n",
    "               guaranteed to) correspond to a row in a large \n",
    "               stacked matrix of features.\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    for n in G.neighbors(n):\n",
    "        indices.append(G.node[n]['idx'])\n",
    "    return indices\n",
    "\n",
    "\n",
    "# test neighbor_indices\n",
    "nbr_idxs = neighbor_indices(syngraphs[0], syngraphs[0].nodes()[0])\n",
    "nbr_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a function that sums each of the neighbors' activations for a\n",
    "# given node in a given graph.\n",
    "def neighbor_activations(G, n, activations_dict, layer):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    =======\n",
    "    - G: the graph to which the node belongs to.\n",
    "    - n: the node inside the graph G\n",
    "    - activations_dict: a dictionary that stores the node activations \n",
    "                        at each layer.\n",
    "    - layer: the layer at which to compute neighbor activations.\n",
    "    \"\"\"\n",
    "    nbr_indices = neighbor_indices(G, n)\n",
    "    return np.sum(activations_dict[layer][nbr_indices], axis=0)\n",
    "\n",
    "neighbor_activations(syngraphs[0], syngraphs[0].nodes()[0], layers, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 1, 0, ..., 0, 1, 1],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 1, 0, ..., 1, 0, 0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a function that stacks each of the nodes' neighbors\n",
    "# activations together into a large feature matrix.\n",
    "\n",
    "def stacked_neighbor_activations(graphs, activations_dict, layer):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    =======\n",
    "    - graphs: (list) a list of NetworkX graphs.\n",
    "    - activations_dict: (dict) a dictionary where keys are the layer\n",
    "                        number and values are the node activations.\n",
    "    \n",
    "    Returns:\n",
    "    ========\n",
    "    - a stacked numpy array of neighbor activations\n",
    "    \"\"\"\n",
    "    nbr_activations = []\n",
    "    for g in graphs:\n",
    "        for n in g.nodes():\n",
    "            nbr_acts = neighbor_activations(g, n, activations_dict, layer)\n",
    "            nbr_activations.append(nbr_acts)\n",
    "    return np.vstack(nbr_activations)\n",
    "\n",
    "stacked_neighbor_activations(syngraphs, layers, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08077149  0.26617627 -0.         ..., -0.         -0.          0.06247178]\n",
      " [-0.         -0.          0.17111185 ...,  0.1100053   0.10972534\n",
      "   0.06213238]\n",
      " [-0.          0.06887595 -0.         ..., -0.         -0.          0.42113589]\n",
      " ..., \n",
      " [ 0.09517687  0.33361817  0.02114029 ...,  0.01350959 -0.          0.3693808 ]\n",
      " [ 0.03914639  0.07409041 -0.         ..., -0.          0.01219503\n",
      "   0.35051585]\n",
      " [-0.         -0.         -0.         ..., -0.         -0.          0.07927655]]\n"
     ]
    }
   ],
   "source": [
    "# Write a function that computes the next layers' activations.\n",
    "\n",
    "def activation(activations_dict, wb, layer, graphs):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    =======\n",
    "    - activations_dict: (dict) a dictionary where keys are the layer\n",
    "                        number and values are the node activations.\n",
    "    - wb: (wb.WeightsAndBiases) the WB class storing the weights and\n",
    "          biases.\n",
    "    - layer: (int) the layer for which to compute the activations.    \n",
    "    \n",
    "    Returns:\n",
    "    ========\n",
    "    - a stacked numpy array of activations, which can be assigned to\n",
    "      the activations_dict's next layer if desired (actually it\n",
    "      should be).\n",
    "    \"\"\"\n",
    "    \n",
    "    self_acts = activations_dict[layer]\n",
    "    self_acts = np.dot(self_acts, wb[layer]['self_weights'])\n",
    "\n",
    "    nbr_acts = stacked_neighbor_activations(graphs, activations_dict, layer)\n",
    "    # print('nbr_dtype: {0}....... wb_dtype: {1}'.format(nbr_acts.dtype, wb[layer]['nbr_weights'].dtype))\n",
    "    # print('nbr_act type: {0}'.format(type(nbr_acts)))\n",
    "    # print('nbr_acts:')\n",
    "    # print(nbr_acts)\n",
    "    # print('nbr_weights:')\n",
    "    # print(wb[layer]['nbr_weights'])\n",
    "    nbr_acts = np.dot(nbr_acts, wb[layer]['nbr_weights'])\n",
    "    # print(nbr_acts)\n",
    "    \n",
    "    biases = wb[layer]['biases']\n",
    "    # print('result_activation dtype: {0}'.format((self_acts + nbr_acts + biases).dtype))\n",
    "    # print('result')\n",
    "    # print(self_acts + nbr_acts + biases)\n",
    "    return relu(self_acts + nbr_acts + biases)\n",
    "\n",
    "print(activation(layers, wb, 0, syngraphs))\n",
    "# print(activation(layers, wb, 1, syngraphs))\n",
    "# print(activation(layers, wb, 2, syngraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(561, 20)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act = np.dot(stacked_neighbor_activations(syngraphs, layers, 0), wb[0]['nbr_weights']) + wb[0]['biases']\n",
    "act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function that gets the indices of all of the nodes in the\n",
    "# graph.\n",
    "def graph_indices(g):\n",
    "    \"\"\"\n",
    "    Returns the row indices of each of the nodes in the graphs.\n",
    "    \"\"\"\n",
    "    return [d['idx'] for _, d in g.nodes(data=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 1, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 0, 1, 1, 0, 1, 1],\n",
       "       [1, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 1, 1, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 1, 0, 1, 1, 1, 0, 1, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 0, 1, 0, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 1, 1, 1, 0, 1, 1],\n",
       "       [1, 1, 1, 1, 0, 0, 1, 1, 1, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 0, 1, 1],\n",
       "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 1, 1, 0, 0, 1, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 1, 0, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 1, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 1, 1, 1, 1, 1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 1, 0, 0, 1, 0, 1, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 1, 1],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 1],\n",
       "       [1, 1, 0, 0, 1, 1, 0, 0, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 0, 0, 1, 1, 1, 0, 1, 1, 1],\n",
       "       [1, 1, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [1, 1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 0, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 0, 0, 1, 1, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 1, 0, 0, 1, 1, 0, 1, 0],\n",
       "       [1, 1, 0, 1, 1, 1, 0, 0, 1, 1],\n",
       "       [1, 1, 0, 1, 1, 1, 1, 0, 1, 1],\n",
       "       [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 1, 1, 1, 1, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 1, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 1, 0, 1, 1, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 1, 1, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 0, 1, 1, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 0, 1, 1, 0, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 0, 0, 1, 1, 1, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 1, 1],\n",
       "       [1, 0, 1, 0, 0, 1, 0, 0, 1, 1],\n",
       "       [1, 0, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 0, 1, 1, 1, 0, 1],\n",
       "       [0, 0, 1, 0, 1, 0, 1, 1, 0, 0],\n",
       "       [1, 1, 0, 1, 1, 0, 1, 1, 1, 1],\n",
       "       [1, 1, 0, 0, 0, 0, 1, 0, 1, 1],\n",
       "       [1, 1, 1, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 1, 1, 0, 1, 1],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [1, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [1, 1, 1, 1, 0, 0, 1, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 1],\n",
       "       [0, 1, 1, 1, 0, 1, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a function that makes the fingerprint used for predictions.\n",
    "def fingerprint(activations_dict, graphs):\n",
    "    \"\"\"\n",
    "    Computes the final layer fingerprint for each graph.\n",
    "    \n",
    "    Inputs:\n",
    "    =======\n",
    "    - activations_dict: (dict) a dictionary where keys are the layer\n",
    "                        number and values are the node activations.\n",
    "    - graphs: a list of graphs for which to compute the fingerprints.\n",
    "    \n",
    "    Returns:\n",
    "    ========\n",
    "    - a stacked numpy array of fingerprints, of length len(graphs).\n",
    "    \"\"\"\n",
    "    top_layer = max(activations_dict.keys())\n",
    "    fingerprints = []\n",
    "    for g in graphs:\n",
    "        idxs = graph_indices(g)\n",
    "        fp = np.sum(activations_dict[top_layer][idxs], axis=0)\n",
    "        fingerprints.append(fp)\n",
    "    \n",
    "    return relu(np.vstack(fingerprints))\n",
    "\n",
    "# test fingerprint function\n",
    "fingerprint(layers, syngraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03521828],\n",
       "       [-0.50638317],\n",
       "       [-0.08300308],\n",
       "       [-0.04979961],\n",
       "       [-0.07955716],\n",
       "       [-0.26432701],\n",
       "       [-1.43592535],\n",
       "       [-0.16698255],\n",
       "       [-0.09445827],\n",
       "       [-0.1593711 ],\n",
       "       [-0.32426517],\n",
       "       [-0.06252155],\n",
       "       [-0.03428062],\n",
       "       [-1.31626076],\n",
       "       [-0.50621963],\n",
       "       [-0.96950534],\n",
       "       [-0.0397043 ],\n",
       "       [-0.17819871],\n",
       "       [-0.25704755],\n",
       "       [-0.15071348],\n",
       "       [-0.16114747],\n",
       "       [-0.06814167],\n",
       "       [-0.07055512],\n",
       "       [-0.08307516],\n",
       "       [-0.03690267],\n",
       "       [-0.06135548],\n",
       "       [-0.08243147],\n",
       "       [-0.48955387],\n",
       "       [-0.07655244],\n",
       "       [-0.3211851 ],\n",
       "       [-0.74178985],\n",
       "       [-0.33583623],\n",
       "       [-0.23711611],\n",
       "       [-0.06503832],\n",
       "       [-0.13641978],\n",
       "       [-0.37008644],\n",
       "       [-0.14108591],\n",
       "       [-0.03261727],\n",
       "       [-0.20669762],\n",
       "       [-0.07996231],\n",
       "       [-0.06615539],\n",
       "       [-0.03427786],\n",
       "       [-0.28221307],\n",
       "       [-0.09265613],\n",
       "       [-0.18684824],\n",
       "       [-0.63184883],\n",
       "       [-1.00867587],\n",
       "       [-0.03494181],\n",
       "       [-0.28617041],\n",
       "       [-0.20409382],\n",
       "       [-0.13146604],\n",
       "       [-0.15770243],\n",
       "       [-0.45769747],\n",
       "       [-1.06488309],\n",
       "       [-0.17065455],\n",
       "       [-0.03474631],\n",
       "       [-0.23610311],\n",
       "       [-0.04542394],\n",
       "       [-0.42824561],\n",
       "       [-0.57381347],\n",
       "       [-0.33161499],\n",
       "       [-0.42069842],\n",
       "       [-0.10009573],\n",
       "       [-0.08315313],\n",
       "       [-0.0947881 ],\n",
       "       [-0.03336425],\n",
       "       [-0.03682855],\n",
       "       [-0.03474631],\n",
       "       [-0.43830699],\n",
       "       [-0.03756216],\n",
       "       [-0.50240922],\n",
       "       [-0.38619137],\n",
       "       [-0.37576237],\n",
       "       [-0.25522471],\n",
       "       [-0.64255483],\n",
       "       [-0.21239685],\n",
       "       [-0.25481923],\n",
       "       [-0.13992136],\n",
       "       [-0.55470471],\n",
       "       [-0.06531929],\n",
       "       [-0.63606779],\n",
       "       [-0.29882019],\n",
       "       [-1.33483358],\n",
       "       [-0.0383184 ],\n",
       "       [-0.10169224],\n",
       "       [-0.10561902],\n",
       "       [-0.69815816],\n",
       "       [-0.24196675],\n",
       "       [-0.26273266],\n",
       "       [-0.10824089],\n",
       "       [-0.07940421],\n",
       "       [-0.61899482],\n",
       "       [-0.15645429],\n",
       "       [-1.1587263 ],\n",
       "       [-0.05783125],\n",
       "       [-0.31786388],\n",
       "       [-0.56960614],\n",
       "       [-1.54070556],\n",
       "       [-0.60196563],\n",
       "       [-0.18389464]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a function that makes the forward pass predictions.\n",
    "def predict(wb_vect, wb_unflattener, activations_dict, graphs):\n",
    "    \"\"\"\n",
    "    Makes predictions.\n",
    "    \n",
    "    Change this function for each new learning problem.\n",
    "    \n",
    "    Inputs:\n",
    "    =======\n",
    "    - wb_vect: (WeightsAndBiases.vect)\n",
    "    - wb_unfalttener (WeightsAndBiases.unflattener)\n",
    "    - activations_dict: (dict) a dictionary where keys are the layer\n",
    "                        number and values are the node activations.\n",
    "    - graphs: a list of graphs for which to compute the fingerprints.\n",
    "    \n",
    "    Returns:\n",
    "    ========\n",
    "    - a numpy array of predictions, of length len(graphs).\n",
    "    \"\"\"\n",
    "    \n",
    "    wb = wb_unflattener(wb_vect)\n",
    "    for k in sorted(wb.keys()):\n",
    "        activations_dict[k + 1] = activation(activations_dict, wb, k, graphs)\n",
    "        # print(activations_dict[k])\n",
    "    \n",
    "    top_layer = max(wb.keys())\n",
    "    \n",
    "    fps = fingerprint(layers, graphs)\n",
    "    \n",
    "    return np.dot(fps, wb[top_layer]['linweights'])\n",
    "\n",
    "predict(*wb.flattened(), layers, syngraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.9175226935169913"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a function that computes the training loss.\n",
    "def train_loss(wb_vect, wb_unflattener, activations_dict, graphs):\n",
    "    \"\"\"\n",
    "    Computes the training loss as mean squared error.\n",
    "    \n",
    "    Inputs:\n",
    "    =======\n",
    "    - wb_vect: (WeightsAndBiases.vect)\n",
    "    - wb_unfalttener (WeightsAndBiases.unflattener)\n",
    "    - activations_dict: (dict) a dictionary where keys are the layer\n",
    "                        number and values are the node activations.\n",
    "    - graphs: a list of graphs for which to compute the fingerprints.\n",
    "\n",
    "    Returns:\n",
    "    ========\n",
    "    - mean squared error.\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = np.array([score(g) for g in graphs]).reshape((len(graphs), 1))\n",
    "    # print(scores)\n",
    "    preds = predict(wb_vect, wb_unflattener, activations_dict, graphs)\n",
    "    # print(preds)\n",
    "    return np.sum(np.abs(preds - scores)) / len(scores)\n",
    "\n",
    "train_loss(wb.vect, wb.unflattener, layers, syngraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.34161722,  0.21893978,  0.00538399, ..., -0.87374541,\n",
       "       -1.01269378, -0.06060852])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradfunc = grad(train_loss, argnum=0)\n",
    "gradfunc(wb.vect, wb.unflattener, layers, syngraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5.85073881171\n",
      "1\n",
      "5.84457159019\n",
      "2\n",
      "5.83582307169\n",
      "3\n",
      "5.82478564025\n",
      "4\n",
      "5.81177303917\n",
      "5\n",
      "5.79686245053\n",
      "6\n",
      "5.78028589435\n",
      "7\n",
      "5.76231856991\n",
      "8\n",
      "5.74324312263\n",
      "9\n",
      "5.72311197145\n",
      "10\n",
      "5.70196153215\n",
      "11\n",
      "5.67982872567\n",
      "12\n",
      "5.65671583035\n",
      "13\n",
      "5.63275326223\n",
      "14\n",
      "5.60800721181\n",
      "15\n",
      "5.58237205945\n",
      "16\n",
      "5.55577868215\n",
      "17\n",
      "5.52810898695\n",
      "18\n",
      "5.49950475366\n",
      "19\n",
      "5.47006171089\n",
      "20\n",
      "5.44004945322\n",
      "21\n",
      "5.40929787154\n",
      "22\n",
      "5.37768854822\n",
      "23\n",
      "5.3449782691\n",
      "24\n",
      "5.31100221178\n",
      "25\n",
      "5.27561676606\n",
      "26\n",
      "5.23907629225\n",
      "27\n",
      "5.2011369265\n",
      "28\n",
      "5.16169450978\n",
      "29\n",
      "5.12043310163\n",
      "30\n",
      "5.07717107834\n",
      "31\n",
      "5.03171420892\n",
      "32\n",
      "4.98391111933\n",
      "33\n",
      "4.9335082699\n",
      "34\n",
      "4.88022602775\n",
      "35\n",
      "4.82373864795\n",
      "36\n",
      "4.76371707327\n",
      "37\n",
      "4.69985876357\n",
      "38\n",
      "4.6316825337\n",
      "39\n",
      "4.55882356348\n",
      "40\n",
      "4.48080836423\n",
      "41\n",
      "4.39713529383\n",
      "42\n",
      "4.30722642362\n",
      "43\n",
      "4.21023091831\n",
      "44\n",
      "4.11810236746\n",
      "45\n",
      "4.03334256909\n",
      "46\n",
      "3.95787460109\n",
      "47\n",
      "3.9068731395\n",
      "48\n",
      "3.87649742704\n",
      "49\n",
      "3.8522175501\n",
      "50\n",
      "3.82833776435\n",
      "51\n",
      "3.80486714752\n",
      "52\n",
      "3.78170948231\n",
      "53\n",
      "3.77239569646\n",
      "54\n",
      "3.7701615418\n",
      "55\n",
      "3.7695829148\n",
      "56\n",
      "3.7722154011\n",
      "57\n",
      "3.77391571285\n",
      "58\n",
      "3.77805107965\n",
      "59\n",
      "3.78203512024\n",
      "60\n",
      "3.786689786\n",
      "61\n",
      "3.78836556652\n",
      "62\n",
      "3.78708533597\n",
      "63\n",
      "3.78312964278\n",
      "64\n",
      "3.77678250966\n",
      "65\n",
      "3.76832695612\n",
      "66\n",
      "3.75801866002\n",
      "67\n",
      "3.74625138644\n",
      "68\n",
      "3.73640113651\n",
      "69\n",
      "3.72715523091\n",
      "70\n",
      "3.71779047607\n",
      "71\n",
      "3.709922064\n",
      "72\n",
      "3.70259489377\n",
      "73\n",
      "3.69536536395\n",
      "74\n",
      "3.68822400363\n",
      "75\n",
      "3.68117057658\n",
      "76\n",
      "3.6742103542\n",
      "77\n",
      "3.66907523287\n",
      "78\n",
      "3.66410515634\n",
      "79\n",
      "3.65910533047\n",
      "80\n",
      "3.65407569014\n",
      "81\n",
      "3.64901101483\n",
      "82\n",
      "3.64391588514\n",
      "83\n",
      "3.63971271651\n",
      "84\n",
      "3.63533969914\n",
      "85\n",
      "3.63070285998\n",
      "86\n",
      "3.62595111403\n",
      "87\n",
      "3.6208548616\n",
      "88\n",
      "3.61513853557\n",
      "89\n",
      "3.60898950235\n",
      "90\n",
      "3.60286920771\n",
      "91\n",
      "3.59656579946\n",
      "92\n",
      "3.59035166071\n",
      "93\n",
      "3.58460304305\n",
      "94\n",
      "3.57878043461\n",
      "95\n",
      "3.57286248093\n",
      "96\n",
      "3.56687054816\n",
      "97\n",
      "3.56174711393\n",
      "98\n",
      "3.55663552697\n",
      "99\n",
      "3.55139562169\n",
      "100\n",
      "3.54603089709\n",
      "101\n",
      "3.54056049382\n",
      "102\n",
      "3.53497864803\n",
      "103\n",
      "3.52929566635\n",
      "104\n",
      "3.52352236806\n",
      "105\n",
      "3.517672028\n",
      "106\n",
      "3.5117490603\n",
      "107\n",
      "3.50577289024\n",
      "108\n",
      "3.49975212106\n",
      "109\n",
      "3.49368342436\n",
      "110\n",
      "3.48799518502\n",
      "111\n",
      "3.48254198593\n",
      "112\n",
      "3.47704334995\n",
      "113\n",
      "3.47149949285\n",
      "114\n",
      "3.46602902667\n",
      "115\n",
      "3.46028329812\n",
      "116\n",
      "3.45442292031\n",
      "117\n",
      "3.44858248854\n",
      "118\n",
      "3.44268427839\n",
      "119\n",
      "3.43672962252\n",
      "120\n",
      "3.43069610434\n",
      "121\n",
      "3.42460550898\n",
      "122\n",
      "3.41846385377\n",
      "123\n",
      "3.41224840643\n",
      "124\n",
      "3.40596244203\n",
      "125\n",
      "3.39961007422\n",
      "126\n",
      "3.39317285329\n",
      "127\n",
      "3.38676427147\n",
      "128\n",
      "3.38045317706\n",
      "129\n",
      "3.37403638337\n",
      "130\n",
      "3.36751641196\n",
      "131\n",
      "3.36088597633\n",
      "132\n",
      "3.35416158989\n",
      "133\n",
      "3.34738465394\n",
      "134\n",
      "3.34052917941\n",
      "135\n",
      "3.33378358718\n",
      "136\n",
      "3.32703461081\n",
      "137\n",
      "3.32020331668\n",
      "138\n",
      "3.31327162026\n",
      "139\n",
      "3.30625017994\n",
      "140\n",
      "3.2991362856\n",
      "141\n",
      "3.2919272826\n",
      "142\n",
      "3.28551708844\n",
      "143\n",
      "3.27884055467\n",
      "144\n",
      "3.27188821879\n",
      "145\n",
      "3.26477408271\n",
      "146\n",
      "3.25778942555\n",
      "147\n",
      "3.25074023936\n",
      "148\n",
      "3.24363013348\n",
      "149\n",
      "3.23646778348\n",
      "150\n",
      "3.22924668059\n",
      "151\n",
      "3.2219625506\n",
      "152\n",
      "3.21461580742\n",
      "153\n",
      "3.207205939\n",
      "154\n",
      "3.19973663092\n",
      "155\n",
      "3.19220924393\n",
      "156\n",
      "3.18461872388\n",
      "157\n",
      "3.17696032477\n",
      "158\n",
      "3.16923027942\n",
      "159\n",
      "3.16142485229\n",
      "160\n",
      "3.15354105257\n",
      "161\n",
      "3.14558256276\n",
      "162\n",
      "3.13753557682\n",
      "163\n",
      "3.12941862238\n",
      "164\n",
      "3.12133168704\n",
      "165\n",
      "3.11303104298\n",
      "166\n",
      "3.10459142279\n",
      "167\n",
      "3.09614109267\n",
      "168\n",
      "3.08760063001\n",
      "169\n",
      "3.07896829391\n",
      "170\n",
      "3.0702355348\n",
      "171\n",
      "3.06141548713\n",
      "172\n",
      "3.05252975453\n",
      "173\n",
      "3.04355069798\n",
      "174\n",
      "3.03446375451\n",
      "175\n",
      "3.02526671917\n",
      "176\n",
      "3.01597698739\n",
      "177\n",
      "3.00655863632\n",
      "178\n",
      "2.99702247448\n",
      "179\n",
      "2.98736408504\n",
      "180\n",
      "2.97759162708\n",
      "181\n",
      "2.96769691766\n",
      "182\n",
      "2.95768033303\n",
      "183\n",
      "2.94753796626\n",
      "184\n",
      "2.93728170543\n",
      "185\n",
      "2.9269172208\n",
      "186\n",
      "2.9164281996\n",
      "187\n",
      "2.90579043332\n",
      "188\n",
      "2.89502244515\n",
      "189\n",
      "2.88435468096\n",
      "190\n",
      "2.87380475399\n",
      "191\n",
      "2.86303622795\n",
      "192\n",
      "2.85203620515\n",
      "193\n",
      "2.8417353541\n",
      "194\n",
      "2.83172004118\n",
      "195\n",
      "2.82166350143\n",
      "196\n",
      "2.81106802418\n",
      "197\n",
      "2.80062276943\n",
      "198\n",
      "2.79119915869\n",
      "199\n",
      "2.78139589005\n"
     ]
    }
   ],
   "source": [
    "def sgd(grad, wb_vect, wb_unflattener, activations_dict, graphs, callback=None, num_iters=200, step_size=0.1, mass=0.9):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent with momentum.\n",
    "    \"\"\"\n",
    "    velocity = np.zeros(len(wb_vect))\n",
    "    for i in range(num_iters):\n",
    "        print(i)\n",
    "        g = grad(wb_vect, wb_unflattener, activations_dict, graphs)\n",
    "\n",
    "        velocity = mass * velocity - (1.0 - mass) * g\n",
    "        wb_vect += step_size * velocity\n",
    "        # print(wb_vect)\n",
    "        print(train_loss(wb_vect, wb_unflattener, activations_dict, graphs))\n",
    "    return wb_vect, wb_unflattener\n",
    "\n",
    "wb_vect, wb_unflattener = sgd(gradfunc, wb.vect, wb.unflattener, layers, syngraphs, num_iters=200, step_size=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.7804210300155514"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss(wb_vect, wb.unflattener, layers, syngraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13523469],\n",
       "       [ 0.0693613 ],\n",
       "       [ 0.07628269],\n",
       "       [ 0.03652902],\n",
       "       [ 0.13936013],\n",
       "       [ 0.02577675],\n",
       "       [-0.02201426],\n",
       "       [ 0.07041358],\n",
       "       [-0.00626544],\n",
       "       [ 0.00346891]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb.unflattener(wb.vect)[2]['linweights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = [score(g) for g in syngraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = predict(wb_vect, wb.unflattener, layers, syngraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, array([ 0.19499841])),\n",
       " (8, array([ 0.9013011])),\n",
       " (5, array([ 0.29641369])),\n",
       " (9, array([ 2.61036359])),\n",
       " (2, array([ 0.00708888])),\n",
       " (5, array([ 0.37208714])),\n",
       " (5, array([ 0.37169495])),\n",
       " (2, array([ 0.01057891])),\n",
       " (2, array([ 0.00726456])),\n",
       " (3, array([ 0.02077415])),\n",
       " (6, array([ 0.25659966])),\n",
       " (2, array([ 0.01021909])),\n",
       " (9, array([ 3.24096215])),\n",
       " (6, array([ 0.94539905])),\n",
       " (5, array([ 0.47199414])),\n",
       " (5, array([ 0.15118387])),\n",
       " (4, array([ 0.23316094])),\n",
       " (3, array([ 0.07847319])),\n",
       " (3, array([ 0.01973731])),\n",
       " (7, array([ 0.49615244])),\n",
       " (2, array([ 0.00857779])),\n",
       " (5, array([ 0.10338652])),\n",
       " (6, array([ 0.06584597])),\n",
       " (6, array([ 0.69345539])),\n",
       " (5, array([ 0.12810961])),\n",
       " (6, array([ 0.14935139])),\n",
       " (2, array([ 0.0101271])),\n",
       " (7, array([ 0.11523734])),\n",
       " (8, array([ 0.0522055])),\n",
       " (5, array([ 0.4975073])),\n",
       " (2, array([ 0.00612571])),\n",
       " (6, array([ 0.75824486])),\n",
       " (5, array([ 0.23375768])),\n",
       " (8, array([ 2.0248615])),\n",
       " (7, array([ 1.08374248])),\n",
       " (5, array([ 0.31255017])),\n",
       " (5, array([ 0.13616035])),\n",
       " (2, array([ 0.00676885])),\n",
       " (7, array([ 0.26601333])),\n",
       " (6, array([ 0.10484524])),\n",
       " (2, array([ 0.00572263])),\n",
       " (9, array([ 2.95913417])),\n",
       " (4, array([ 0.01895998])),\n",
       " (3, array([ 0.01530585])),\n",
       " (7, array([ 0.02532374])),\n",
       " (3, array([ 0.05231339])),\n",
       " (9, array([ 3.58971862])),\n",
       " (6, array([ 0.14696956])),\n",
       " (5, array([ 0.40913604])),\n",
       " (6, array([ 0.02003976])),\n",
       " (5, array([ 0.01729848])),\n",
       " (3, array([ 0.01071292])),\n",
       " (6, array([ 0.71398807])),\n",
       " (8, array([ 2.35843297])),\n",
       " (7, array([ 0.03067029])),\n",
       " (3, array([ 0.01174745])),\n",
       " (3, array([ 0.02333851])),\n",
       " (6, array([ 0.85014685])),\n",
       " (2, array([ 0.00996202])),\n",
       " (2, array([ 0.00541504])),\n",
       " (9, array([ 0.0363498])),\n",
       " (7, array([ 0.32882036])),\n",
       " (9, array([ 2.18545028])),\n",
       " (7, array([ 1.59797922])),\n",
       " (2, array([ 0.01096476])),\n",
       " (8, array([ 0.10640027])),\n",
       " (8, array([ 1.87906653])),\n",
       " (5, array([ 0.32598351])),\n",
       " (5, array([ 0.02146026])),\n",
       " (5, array([ 0.30853639])),\n",
       " (5, array([ 0.02482023])),\n",
       " (2, array([ 0.00551556])),\n",
       " (8, array([ 2.07203919])),\n",
       " (2, array([ 0.00976526])),\n",
       " (4, array([ 0.01360216])),\n",
       " (5, array([ 0.01882799])),\n",
       " (4, array([ 0.20042209])),\n",
       " (2, array([ 0.00822592])),\n",
       " (3, array([ 0.02530391])),\n",
       " (5, array([ 0.79325876])),\n",
       " (5, array([ 0.03143198])),\n",
       " (7, array([ 0.08193219])),\n",
       " (6, array([ 0.27895617])),\n",
       " (4, array([ 0.03225263])),\n",
       " (9, array([ 4.25264544])),\n",
       " (4, array([ 0.14486234])),\n",
       " (5, array([ 0.15674809])),\n",
       " (4, array([ 0.07969008])),\n",
       " (6, array([ 0.2637971])),\n",
       " (7, array([ 1.90361607])),\n",
       " (9, array([ 3.50939719])),\n",
       " (8, array([ 1.25815963])),\n",
       " (4, array([ 0.0654194])),\n",
       " (7, array([ 1.57199487])),\n",
       " (8, array([ 1.73524679])),\n",
       " (6, array([ 0.31852292])),\n",
       " (6, array([ 0.03540661])),\n",
       " (5, array([ 0.11811127])),\n",
       " (7, array([ 1.38512764])),\n",
       " (8, array([ 0.0341285]))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in zip(scores, preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 1, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_graphs = [make_random_graph(sample(all_nodes, 4), 5, features_dict) for i in range(100)]\n",
    "# predict(wb_vect, wb.unflattener, layers, new_graphs)\n",
    "new_graphs[0].nodes(data=True)\n",
    "\n",
    "stacked_node_activations(new_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (534,20) (400,20) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-f82372ec046b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwb_vect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflattener\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_graphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-38ccf9e2efb0>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(wb_vect, wb_unflattener, activations_dict, graphs)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mwb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwb_unflattener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwb_vect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mactivations_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# print(activations_dict[k])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-32815cd899ec>\u001b[0m in \u001b[0;36mactivation\u001b[0;34m(activations_dict, wb, layer, graphs)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# print('result')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# print(self_acts + nbr_acts + biases)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_acts\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnbr_acts\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyngraphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (534,20) (400,20) "
     ]
    }
   ],
   "source": [
    "predict(wb_vect, wb.unflattener, layers, new_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
